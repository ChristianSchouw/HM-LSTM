{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearningProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_nvMfJlmAXh",
        "colab_type": "text"
      },
      "source": [
        "# Deep Bioinformatics project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJFILejsh027",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import os\n",
        "import torch\n",
        "import pickle as pkl\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import Linear, GRU, Dropout, BatchNorm1d, LSTM\n",
        "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO9Xzj5qllwk",
        "colab_type": "text"
      },
      "source": [
        "## data import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OeuiP5qlkPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "#from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "#root_path = '/content/drive/My Drive/Deep_stuff/'  #change dir to your project folder\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZt1sdXI3G6Y",
        "colab_type": "text"
      },
      "source": [
        "## Use cuda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMWLPu3j3K_6",
        "colab_type": "code",
        "outputId": "d88fd0b9-3c10-4b56-9894-f6b4f76db40b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(\"Running GPU.\") if use_cuda else print(\"No GPU available.\")\n",
        "\n",
        "\n",
        "def get_variable(x):\n",
        "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
        "    if use_cuda:\n",
        "        return x.cuda()\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_numpy(x):\n",
        "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
        "    if use_cuda:\n",
        "        return x.cpu().data.numpy()\n",
        "    return x.data.numpy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TB4LjdDa2XF",
        "colab_type": "text"
      },
      "source": [
        "## Data (Penn Treebank)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpZrNjHAbCuY",
        "colab_type": "code",
        "outputId": "5ffd117f-ac84-4927-947b-384bb7cf1508",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import GloVe\n",
        "def tokenize(lines):\n",
        "  return [line for line in lines]\n",
        "# Approach 1:\n",
        "# set up fields\n",
        "TEXT = data.Field(lower=True,tokenize=tokenize, batch_first=True)\n",
        "\n",
        "# make splits for data\n",
        "train, valid, test = datasets.PennTreebank.splits(TEXT)\n",
        "\n",
        "# print information about the data\n",
        "print('train.fields', train.fields)\n",
        "print('len(train)', len(train))\n",
        "print('vars(train[0])', vars(train[0])['text'][0:10])\n",
        "\n",
        "# build the vocabulary\n",
        "TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300)) # GloVe is a better version of Word2Vec, it is a general thing. \n",
        "# 100 for the dimension of the embedding is suitable. Better not to use GloVe as it is too general\n",
        "# We will do embeddings also with the protein dataset\n",
        "\n",
        "# print vocab information\n",
        "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
        "\n",
        "vars(train[0])['text'] = vars(train[0])['text'][0:10000]\n",
        "vars(valid[0])['text'] = vars(test[0])['text'][0:2000]\n",
        "vars(test[0])['text'] = vars(test[0])['text'][0:1000]\n",
        "# make iterator for splits\n",
        "train_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n",
        "    #(train, valid, test), batch_size=3, bptt_len=30)\n",
        "    (train, valid, test), batch_size=32, bptt_len=64)\n",
        "\n",
        "# batch_size = number of sentences // bptt (back propagation through time) = number of words per sentence\n",
        "\n",
        "# print batch information\n",
        "batch = next(iter(train_iter))\n",
        "print(vars(train[0])['text'][0:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading ptb.train.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ptb.train.txt: 5.10MB [00:00, 32.3MB/s]\n",
            "ptb.valid.txt:   0%|          | 0.00/135k [00:00<?, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading ptb.valid.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rptb.valid.txt: 400kB [00:00, 18.7MB/s]                   \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading ptb.test.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ptb.test.txt: 450kB [00:00, 18.9MB/s]                   \n",
            ".vector_cache/glove.6B.zip: 0.00B [00:00, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train.fields {'text': <torchtext.data.field.Field object at 0x7f28b226dfd0>}\n",
            "len(train) 1\n",
            "vars(train[0]) [' ', 'a', 'e', 'r', ' ', 'b', 'a', 'n', 'k', 'n']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:26, 2.23MB/s]                          \n",
            "100%|█████████▉| 399188/400000 [00:46<00:00, 8140.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "len(TEXT.vocab) 51\n",
            "[' ', 'a', 'e', 'r', ' ', 'b', 'a', 'n', 'k', 'n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx-BRfyP2sRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#vars(train[0])['text'] = vars(train[0])['text'][0:50000]\n",
        "#print(len(vars(train[0])['text']))\n",
        "# print(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8ngcxc-ZD5a",
        "colab_type": "text"
      },
      "source": [
        "HM_LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRNoR6oIZEeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Function, Variable\n",
        "import torch.nn.functional as Func\n",
        "from torch.nn import Module, Parameter\n",
        "import math\n",
        "import time\n",
        "import torch.optim as optim\n",
        "import numpy\n",
        "import torch.nn.functional as Func\n",
        "import os\n",
        "#from utils import reverse, batchify, get_batch, repackage_hidden, evaluatePTB\n",
        "\n",
        "hidden_dim = 200\n",
        "vocab_size = len(TEXT.vocab)\n",
        "bptt_len = 64\n",
        "embedding_size = 300\n",
        "out_dim = 500\n",
        "batch_size = 32\n",
        "def repackage_hidden(h):\n",
        "    if type(h) == Variable:\n",
        "        return Variable(h.data)\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "def hard_sigm(a, x):\n",
        "    temp = torch.div(torch.add(torch.mul(x, a), 1), 2.0)\n",
        "    output = torch.clamp(temp, min=0, max=1)\n",
        "    return output\n",
        "\n",
        "class bound(Function):\n",
        "    def forward(self, x):\n",
        "        # forward : x -> output\n",
        "        self.save_for_backward(x)\n",
        "        output = x > 0.5\n",
        "        return output.float()\n",
        "\n",
        "    def backward(self, output_grad):\n",
        "        # backward: output_grad -> x_grad\n",
        "        x = self.saved_tensors\n",
        "        x_grad = None\n",
        "\n",
        "        if self.needs_input_grad[0]:\n",
        "            x_grad = output_grad.clone()\n",
        "\n",
        "        return x_grad\n",
        "\n",
        "class masked_NLLLoss(Module):\n",
        "    def __init__(self):\n",
        "        super(masked_NLLLoss, self).__init__()\n",
        "\n",
        "    def forward(self, cost, inputs, mask):\n",
        "        # inputs.size = mask.size = batch_size\n",
        "        # cost.size = batch_size * dict_size\n",
        "\n",
        "        # The following version is too slow, deprecated now\n",
        "        # loss = Variable(torch.zeros(inputs.size(0))).cuda()\n",
        "        # for i in range(inputs.size(0)):\n",
        "        #     loss[i] = - cost[i, inputs[i]] * mask[i]\n",
        "\n",
        "        # The following version is much faster\n",
        "        batch_size, dict_size = cost.size()\n",
        "        cost_flat = cost.view(batch_size*dict_size)\n",
        "        inputs_flat = torch.arange(0, inputs.size(0)).cuda().long()\n",
        "        inputs_flat_idx = torch.mul(inputs_flat, dict_size) + inputs\n",
        "        loss = -cost_flat[inputs_flat_idx] * mask\n",
        "        return loss\n",
        "\n",
        "class HM_LSTMCell(Module):\n",
        "    def __init__(self, bottom_size, hidden_size, top_size, a, last_layer):\n",
        "        super(HM_LSTMCell, self).__init__()\n",
        "        self.bottom_size = bottom_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.top_size = top_size\n",
        "        self.a = a\n",
        "        self.last_layer = last_layer\n",
        "        '''\n",
        "        U_11 means the state transition parameters from layer l (current layer) to layer l\n",
        "        U_21 means the state transition parameters from layer l+1 (top layer) to layer l\n",
        "        W_01 means the state transition parameters from layer l-1 (bottom layer) to layer l\n",
        "        '''\n",
        "        self.U_11 = Parameter(torch.cuda.FloatTensor(4 * self.hidden_size + 1, self.hidden_size))\n",
        "        if not self.last_layer:\n",
        "            self.U_21 = Parameter(torch.cuda.FloatTensor(4 * self.hidden_size + 1, self.top_size))\n",
        "        self.W_01 = Parameter(torch.cuda.FloatTensor(4 * self.hidden_size + 1, self.bottom_size))\n",
        "        self.bias = Parameter(torch.cuda.FloatTensor(4 * self.hidden_size + 1))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "        for par in self.parameters():\n",
        "            par.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, c, h_bottom, h, h_top, z, z_bottom):\n",
        "        # h_bottom.size = bottom_size * batch_size\n",
        "        s_recur = torch.mm(self.W_01, h_bottom)\n",
        "        if not self.last_layer:\n",
        "            s_topdown_ = torch.mm(self.U_21, h_top)\n",
        "            s_topdown = z.expand_as(s_topdown_) * s_topdown_\n",
        "        else:\n",
        "            s_topdown = Variable(torch.zeros(s_recur.size()).cuda(), requires_grad=False).cuda()\n",
        "        s_bottomup_ = torch.mm(self.U_11, h)\n",
        "        s_bottomup = z_bottom.expand_as(s_bottomup_) * s_bottomup_\n",
        "\n",
        "        f_s = s_recur + s_topdown + s_bottomup + self.bias.unsqueeze(1).expand_as(s_recur)\n",
        "        # f_s.size = (4 * hidden_size + 1) * batch_size\n",
        "        f = Func.sigmoid(f_s[0:self.hidden_size, :])  # hidden_size * batch_size\n",
        "        i = Func.sigmoid(f_s[self.hidden_size:self.hidden_size*2, :])\n",
        "        o = Func.sigmoid(f_s[self.hidden_size*2:self.hidden_size*3, :])\n",
        "        g = Func.tanh(f_s[self.hidden_size*3:self.hidden_size*4, :])\n",
        "        z_hat = hard_sigm(self.a, f_s[self.hidden_size*4:self.hidden_size*4+1, :])\n",
        "\n",
        "        one = Variable(torch.ones(f.size()).cuda(), requires_grad=False)\n",
        "        z = z.expand_as(f)\n",
        "        z_bottom = z_bottom.expand_as(f)\n",
        "\n",
        "        c_new = z * (i * g) + (one - z) * (one - z_bottom) * c + (one - z) * z_bottom * (f * c + i * g)\n",
        "        h_new = z * o * Func.tanh(c_new) + (one - z) * (one - z_bottom) * h + (one - z) * z_bottom * o * Func.tanh(c_new)\n",
        "\n",
        "        # if z == 1: (FLUSH)\n",
        "        #     c_new = i * g\n",
        "        #     h_new = o * Func.tanh(c_new)\n",
        "        # elif z_bottom == 0: (COPY)\n",
        "        #     c_new = c\n",
        "        #     h_new = h\n",
        "        # else: (UPDATE)\n",
        "        #     c_new = f * c + i * g\n",
        "        #     h_new = o * Func.tanh(c_new)\n",
        "\n",
        "        z_new = bound()(z_hat)\n",
        "\n",
        "        return h_new, c_new, z_new\n",
        "\n",
        "\n",
        "class HM_LSTM(Module):\n",
        "    def __init__(self, a, input_size, size_list):\n",
        "        super(HM_LSTM, self).__init__()\n",
        "        self.a = a\n",
        "        self.input_size = input_size\n",
        "        self.size_list = size_list\n",
        "\n",
        "        self.cell_1 = HM_LSTMCell(self.input_size, self.size_list[0], self.size_list[1], self.a, False)\n",
        "        self.cell_2 = HM_LSTMCell(self.size_list[0], self.size_list[1], None, self.a, True)\n",
        "\n",
        "    def forward(self, inputs, hidden):\n",
        "        # inputs.size = (batch_size, time steps, embed_size/input_size)\n",
        "        time_steps = inputs.size(1)\n",
        "        batch_size = inputs.size(0)\n",
        "\n",
        "        if hidden == None:\n",
        "            h_t1 = Variable(torch.zeros(self.size_list[0], batch_size).float().cuda(), requires_grad=False)\n",
        "            c_t1 = Variable(torch.zeros(self.size_list[0], batch_size).float().cuda(), requires_grad=False)\n",
        "            z_t1 = Variable(torch.zeros(1, batch_size).float().cuda(), requires_grad=False)\n",
        "            h_t2 = Variable(torch.zeros(self.size_list[1], batch_size).float().cuda(), requires_grad=False)\n",
        "            c_t2 = Variable(torch.zeros(self.size_list[1], batch_size).float().cuda(), requires_grad=False)\n",
        "            z_t2 = Variable(torch.zeros(1, batch_size).float().cuda(), requires_grad=False)\n",
        "        else:\n",
        "            (h_t1, c_t1, z_t1, h_t2, c_t2, z_t2) = hidden\n",
        "        z_one = Variable(torch.ones(1, batch_size).float().cuda(), requires_grad=False)\n",
        "\n",
        "        h_1 = []\n",
        "        h_2 = []\n",
        "        z_1 = []\n",
        "        z_2 = []\n",
        "        for t in range(time_steps):\n",
        "            h_t1, c_t1, z_t1 = self.cell_1(c=c_t1, h_bottom=inputs[:, t, :].t(), h=h_t1, h_top=h_t2, z=z_t1, z_bottom=z_one)\n",
        "            h_t2, c_t2, z_t2 = self.cell_2(c=c_t2, h_bottom=h_t1, h=h_t2, h_top=None, z=z_t2, z_bottom=z_t1)  # 0.01s used\n",
        "            h_1 += [h_t1.t()]\n",
        "            h_2 += [h_t2.t()]\n",
        "            z_1 += [z_t1.t()]\n",
        "            z_2 += [z_t2.t()]\n",
        "\n",
        "        hidden = (h_t1, c_t1, z_t1, h_t2, c_t2, z_t2)\n",
        "        return torch.stack(h_1, dim=1), torch.stack(h_2, dim=1), torch.stack(z_1, dim=1), torch.stack(z_2, dim=1), hidden\n",
        "\n",
        "class HM_Net(Module):\n",
        "    def __init__(self, a, size_list, dict_size, embed_size):\n",
        "        super(HM_Net, self).__init__()\n",
        "        self.dict_size = dict_size\n",
        "        self.size_list = size_list\n",
        "        self.drop = nn.Dropout(p=0.5)\n",
        "        self.embed_in = nn.Embedding(dict_size, embed_size)\n",
        "        self.HM_LSTM = HM_LSTM(a, embed_size, size_list)\n",
        "        self.weight = nn.Linear(size_list[0]+size_list[1], 2)\n",
        "        self.embed_out1 = nn.Linear(size_list[0], dict_size)\n",
        "        self.embed_out2 = nn.Linear(size_list[1], dict_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        # self.logsoftmax = nn.LogSoftmax()\n",
        "        # self.loss = masked_NLLLoss()\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, inputs, target, hidden):\n",
        "        # inputs : batch_size * time_steps\n",
        "        # mask : batch_size * time_steps\n",
        "\n",
        "        emb = self.embed_in(Variable(inputs, volatile=not self.training))  # batch_size * time_steps * embed_size\n",
        "        emb = self.drop(emb)\n",
        "        h_1, h_2, z_1, z_2, hidden = self.HM_LSTM(emb, hidden)  # batch_size * time_steps * hidden_size\n",
        "\n",
        "        # mask = Variable(mask, requires_grad=False)\n",
        "        # batch_loss = Variable(torch.zeros(batch_size).cuda())\n",
        "\n",
        "        h_1 = self.drop(h_1)  # batch_size * time_steps * hidden_size\n",
        "        h_2 = self.drop(h_2)\n",
        "        h = torch.cat((h_1, h_2), 2)\n",
        "\n",
        "        g = Func.sigmoid(self.weight(h.view(h.size(0)*h.size(1), h.size(2))))\n",
        "        g_1 = g[:, 0:1]  # batch_size * time_steps, 1\n",
        "        g_2 = g[:, 1:2]\n",
        "\n",
        "        h_e1 = g_1.expand(g_1.size(0), self.dict_size)*self.embed_out1(h_1.view(h_1.size(0)*h_1.size(1), h_2.size(2)))\n",
        "        h_e2 = g_2.expand(g_2.size(0), self.dict_size)*self.embed_out2(h_2.view(h_2.size(0)*h_2.size(1), h_2.size(2)))\n",
        "\n",
        "        h_e = self.relu(h_e1 + h_e2)  # batch_size*time_steps, hidden_size\n",
        "        # target = target.view(-1).type(torch.cuda.LongTensor)\n",
        "        batch_loss = self.loss(h_e, Variable(target))\n",
        "\n",
        "        return batch_loss, hidden, z_1, z_2\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        h_t1 = Variable(torch.zeros(self.size_list[0], batch_size).float().cuda(), requires_grad=False)\n",
        "        c_t1 = Variable(torch.zeros(self.size_list[0], batch_size).float().cuda(), requires_grad=False)\n",
        "        z_t1 = Variable(torch.zeros(1, batch_size).float().cuda(), requires_grad=False)\n",
        "        h_t2 = Variable(torch.zeros(self.size_list[1], batch_size).float().cuda(), requires_grad=False)\n",
        "        c_t2 = Variable(torch.zeros(self.size_list[1], batch_size).float().cuda(), requires_grad=False)\n",
        "        z_t2 = Variable(torch.zeros(1, batch_size).float().cuda(), requires_grad=False)\n",
        "\n",
        "        hidden = (h_t1, c_t1, z_t1, h_t2, c_t2, z_t2)\n",
        "        return hidden\n",
        "\n",
        "#net = HM_Net(0.5,[200,200],vocab_size, embedding_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGWeZ7CoVtbZ",
        "colab_type": "text"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSo3nHUNVT9c",
        "colab_type": "code",
        "outputId": "b0b8bc71-daa8-4bb5-dd02-bd171f21d85e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "hidden_dim = 200\n",
        "vocab_size = len(TEXT.vocab)\n",
        "bptt_len = 64\n",
        "embedding_size = 100\n",
        "out_dim = 500\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size) #vocab size, vector size\n",
        "        \n",
        "        # Recurrent layer\n",
        "        self.lstm = nn.LSTM(input_size = embedding_size,\n",
        "                           hidden_size = hidden_dim)\n",
        "        self.Dropout = nn.Dropout(0.2)\n",
        "        # Output layer\n",
        "\n",
        "        self.ff = nn.Linear(in_features = hidden_dim,\n",
        "                            out_features = out_dim,\n",
        "                            bias=False)\n",
        "        self.l_out = nn.Linear(in_features = hidden_dim,\n",
        "                            out_features = vocab_size,\n",
        "                            bias=False)\n",
        "        \n",
        "        self.batchnorm = nn.BatchNorm1d(num_features=hidden_dim)\n",
        "\n",
        "        #emb = embedded_dropout(self.encoder, input, dropout=self.dropoute if self.training else 0)\n",
        "        #emb = self.idrop(emb)\n",
        "        #emb = self.lockdrop(emb, self.dropouti)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.cuda()\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # RNN returns output and last hidden state\n",
        "\n",
        "        x, (h, c) = self.lstm(x)\n",
        "        \n",
        "\n",
        "        # Flatten output for feed-forward layer\n",
        "        x = x.view(-1, self.lstm.hidden_size)\n",
        "\n",
        "        x = self.batchnorm(x)\n",
        "\n",
        "        #x = self.ff(x)\n",
        "        #x = self.Dropout(x)\n",
        "        #x = relu(x)\n",
        "        x = self.l_out(x)\n",
        "        #x = relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (embedding): Embedding(51, 100)\n",
            "  (lstm): LSTM(100, 200)\n",
            "  (Dropout): Dropout(p=0.2, inplace=False)\n",
            "  (ff): Linear(in_features=200, out_features=500, bias=False)\n",
            "  (l_out): Linear(in_features=200, out_features=51, bias=False)\n",
            "  (batchnorm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQlYosG1V7iN",
        "colab_type": "text"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehjt8tduV7HR",
        "colab_type": "code",
        "outputId": "dcfab57c-bf91-4c3c-9be1-33855ca98ce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        }
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import torch.optim as optim\n",
        "\n",
        "# Train with cross entropy and at the end evaluate perplexity, similar to an accuracy measure\n",
        "\n",
        "# Hyper-parameters\n",
        "num_epochs = 3\n",
        "batch_size = 32\n",
        "clip = 1\n",
        "# Initialize a new network\n",
        "#net = Net()\n",
        "net = HM_Net(1,[512,512],vocab_size, embedding_size)\n",
        "net = net.cuda()\n",
        "\n",
        "# Define a loss function and optimizer for this problem\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.0003) # ,weight_decay=0.001)\n",
        "#optimizer = optim.SGD(net.parameters(), lr=0.00001)\n",
        "\n",
        "# Track loss\n",
        "training_loss, validation_loss, BPC = [], [], []\n",
        "\n",
        "# For each epoch\n",
        "for i in range(num_epochs):\n",
        "    # Track loss\n",
        "    epoch_training_loss = 0\n",
        "    epoch_validation_loss = 0\n",
        "        \n",
        "    # For each sentence in validation set\n",
        "    #for text, target in valid_iter:\n",
        "    valid_sum=0\n",
        "    hidden = net.init_hidden(batch_size)\n",
        "    # print(hidden)\n",
        "    for j,batch in enumerate(valid_iter):\n",
        "        if j % 1000 ==0:\n",
        "          print(\"batch: \", j , \"out of \", len(valid_iter))\n",
        "        text = batch.text.cuda()\n",
        "        target = batch.target.cuda()\n",
        "        # hidden = repackage_hidden(hidden)\n",
        "        # Forward pass\n",
        "\n",
        "        #target = target.t()\n",
        "        text = text.t()\n",
        "        target = target.view(-1).type(torch.cuda.LongTensor)\n",
        "\n",
        "        net.eval()\n",
        "        output = net(text,target,hidden)\n",
        "        \n",
        "        #print(loss)\n",
        "        #target = target.view(-1).type(torch.cuda.LongTensor)\n",
        "        #target = target.view(-1).type(torch.LongTensor)\n",
        "        # Compute loss\n",
        "        #loss = criterion(outputs, target)\n",
        "        #print(loss[0])\n",
        "        # Update loss\n",
        "        # epoch_validation_loss += loss.detach().cpu().data.numpy()\n",
        "        epoch_validation_loss += output[0]\n",
        "\n",
        "        #valid_sum+=loss\n",
        "    net.train()\n",
        "    \n",
        "    # For each sentence in training set\n",
        "    for j,batch in enumerate(train_iter):\n",
        "        if j % 1000 ==0:\n",
        "          print(\"batch: \", j, \"out of \", len(train_iter))\n",
        "        text = batch.text.cuda()\n",
        "        target = batch.target.cuda()\n",
        "                \n",
        "        # Forward pass\n",
        "        text = text.t()\n",
        "        target = target.view(-1).type(torch.cuda.LongTensor)\n",
        "        output = net(text, target, hidden)\n",
        "        \n",
        "        #target = target.view(-1).type(torch.cuda.LongTensor)\n",
        "        #target = target.view(-1).type(torch.LongTensor)\n",
        "        # Compute loss\n",
        "        #loss = criterion(outputs, target)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        output[0].backward()\n",
        "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Update loss\n",
        "        epoch_training_loss += output[0]\n",
        "\n",
        "    \n",
        "    # Save loss for plot\n",
        "    training_loss.append(epoch_training_loss/len(train_iter))\n",
        "    validation_loss.append(epoch_validation_loss/len(valid_iter))\n",
        "\n",
        "    # Print loss every 5 epochs\n",
        "    \n",
        "    print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
        "\n",
        "\n",
        "    net.HM_LSTM.cell_1.a += 0.04\n",
        "    net.HM_LSTM.cell_2.a += 0.04 \n",
        "\n",
        "# Get first sentence in test set\n",
        "#text, target = test_iter[1]\n",
        "\n",
        "# One-hot encode input and target sequence\n",
        "# inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
        "# targets_idx = [word_to_idx[word] for word in targets]\n",
        "\n",
        "# Convert input to tensor\n",
        "# inputs_one_hot = torch.Tensor(inputs_one_hot)\n",
        "# inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
        "\n",
        "# Convert target to tensor\n",
        "# targets_idx = torch.LongTensor(targets_idx)\n",
        "\n",
        "# Forward pass\n",
        "# outputs = net.forward(text).cpu().data.numpy()\n",
        "\n",
        "#print('\\nInput sequence:')\n",
        "#print(text)\n",
        "\n",
        "#print('\\nTarget sequence:')\n",
        "#print(target)\n",
        "\n",
        "#print('\\nPredicted sequence:')\n",
        "#print([idx_to_word[np.argmax(output)] for output in outputs])\n",
        "\n",
        "# Plot training and validation loss\n",
        "epoch = np.arange(len(training_loss))\n",
        "plt.figure()\n",
        "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
        "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch:  0 out of  1\n",
            "batch:  0 out of  5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 399188/400000 [01:00<00:00, 8140.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, training loss: 3.9195313453674316, validation loss: 3.931668519973755\n",
            "batch:  0 out of  1\n",
            "batch:  0 out of  5\n",
            "Epoch 1, training loss: 3.8737504482269287, validation loss: 3.8919034004211426\n",
            "batch:  0 out of  1\n",
            "batch:  0 out of  5\n",
            "Epoch 2, training loss: 3.7078590393066406, validation loss: 3.787393808364868\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOedi+n+fOxakfi+LNAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYvpKOC-gDll",
        "colab_type": "code",
        "outputId": "ca1fb504-9f40-4166-cfe8-63935a111752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Dec 24 10:33:55 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    32W / 250W |   1657MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcB5NMargFt7",
        "colab_type": "code",
        "outputId": "616edcd3-3a38-4b1d-dff0-7c52c7dd34b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# For each sentence in test set\n",
        "test_sum=0\n",
        "for j,batch in enumerate(test_iter):\n",
        "    text = batch.text.cuda()\n",
        "    target = batch.target.cuda()\n",
        "    \n",
        "    # Forward pass\n",
        "    text = text.t()\n",
        "    target = target.view(-1).type(torch.cuda.LongTensor)\n",
        "    outputs = net(text, target, hidden)\n",
        "\n",
        "\n",
        "    #target = target.view(-1).type(torch.LongTensor)\n",
        "\n",
        "    # Compute loss\n",
        "    #loss = criterion(outputs, target)\n",
        "    \n",
        "    # calculate softmax BPC\n",
        "    #m = target.shape[0]\n",
        "    #p = softmax(outputs.detach().cpu().data.numpy())\n",
        "    #log_likelihood = -np.log2(p[range(m),target.detach().cpu().data.numpy()])\n",
        "    #loss = np.sum(log_likelihood) / m\n",
        "\n",
        "    test_sum += outputs[0]/np.log(2)\n",
        "\n",
        "mean_sum=test_sum/len(test_iter)\n",
        "\n",
        "BPC.append(mean_sum)\n",
        "print(f'Epoch {i}, BPC: {BPC[-1]}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 2, BPC: 5.139392852783203\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
